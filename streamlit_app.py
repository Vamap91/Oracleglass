import streamlit as st
import os
import pypdf
from io import BytesIO
import time
from datetime import datetime
import base64
import pandas as pd
import tiktoken

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="Or√°culo - Sistema de Consulta Inteligente",
    page_icon="üöó",
    layout="wide"
)

# Vari√°veis globais
PDF_PATH = "data/Guia R√°pido.pdf"
VALIDATION_OK = True
VALIDATION_MESSAGES = {}

# Inicializar vari√°veis de sess√£o
if "pdf_text" not in st.session_state:
    st.session_state.pdf_text = ""
if "history" not in st.session_state:
    st.session_state.history = []
if "model" not in st.session_state:
    st.session_state.model = "gpt-3.5-turbo"  # Modelo padr√£o mais econ√¥mico
if "processing_status" not in st.session_state:
    st.session_state.processing_status = None

def validate_environment():
    """Valida todo o ambiente necess√°rio para funcionamento do sistema"""
    global VALIDATION_OK, VALIDATION_MESSAGES
    
    # Resetar estado
    VALIDATION_OK = True
    VALIDATION_MESSAGES = {}
    
    # 1. Validar chave OpenAI - Verifica√ß√£o mais segura
    try:
        import openai
        # Definir API key explicitamente antes de verificar
        openai.api_key = st.secrets["OPENAI_API_KEY"]
        VALIDATION_MESSAGES["openai_key"] = "OK"
    except Exception as e:
        VALIDATION_OK = False
        VALIDATION_MESSAGES["openai_key"] = f"Erro ao verificar API key: {str(e)}"
    
    # 2. Verificar exist√™ncia do PDF
    if not os.path.exists(PDF_PATH):
        VALIDATION_OK = False
        VALIDATION_MESSAGES["pdf_exists"] = f"Arquivo {PDF_PATH} n√£o encontrado. Verifique o reposit√≥rio GitHub."
    else:
        VALIDATION_MESSAGES["pdf_exists"] = "OK"
    
    # 3. Validar extra√ß√£o de texto
    if VALIDATION_MESSAGES.get("pdf_exists") == "OK":
        try:
            st.session_state.pdf_text = extract_text_from_pdf(PDF_PATH)
            if not st.session_state.pdf_text or len(st.session_state.pdf_text) < 10:
                VALIDATION_OK = False
                VALIDATION_MESSAGES["text_extraction"] = "N√£o foi poss√≠vel extrair texto v√°lido do PDF."
            else:
                VALIDATION_MESSAGES["text_extraction"] = "OK"
        except Exception as e:
            VALIDATION_OK = False
            VALIDATION_MESSAGES["text_extraction"] = f"Erro ao extrair texto: {str(e)}"
    else:
        VALIDATION_OK = False
        VALIDATION_MESSAGES["text_extraction"] = "N√£o foi poss√≠vel extrair texto sem o arquivo PDF."
    
    # 4. Verificar depend√™ncias
    try:
        import pypdf
        import openai
        import tiktoken
        VALIDATION_MESSAGES["dependencies"] = "OK"
    except ImportError as e:
        VALIDATION_OK = False
        VALIDATION_MESSAGES["dependencies"] = f"Depend√™ncia faltando: {str(e)}"
    
    return VALIDATION_OK

def extract_text_from_pdf(pdf_path):
    """Extrai texto de um arquivo PDF com m√©todo melhorado para preservar estrutura"""
    text_content = ""
    try:
        # Abrir o arquivo PDF
        reader = pypdf.PdfReader(pdf_path)
        
        # Iterar por todas as p√°ginas
        for page_num, page in enumerate(reader.pages):
            # Extrair o texto com configura√ß√µes para preservar layout
            page_text = page.extract_text() or ""
            
            # Adicionar n√∫mero da p√°gina e texto
            text_content += f"\n--- P√°gina {page_num+1} ---\n{page_text}\n"
            
            # Tentar extrair tabelas e conte√∫do estruturado (abordagem simples)
            lines = page_text.split('\n')
            for i, line in enumerate(lines):
                # Detectar poss√≠veis n√∫meros de telefone
                if ('0800' in line or '4004' in line or 
                    'telefone' in line.lower() or 'contato' in line.lower() or
                    '-' in line and any(c.isdigit() for c in line)):
                    # Adicionar linhas ao redor para contexto
                    start_idx = max(0, i-2)
                    end_idx = min(len(lines), i+3)
                    context_lines = lines[start_idx:end_idx]
                    text_content += "\n--- INFORMA√á√ÉO DE CONTATO DETECTADA ---\n"
                    text_content += "\n".join(context_lines) + "\n"
                    text_content += "--- FIM DA INFORMA√á√ÉO DE CONTATO ---\n"
    
    except Exception as e:
        st.error(f"Erro ao processar PDF: {str(e)}")
    
    return text_content

def estimate_tokens(text, model="gpt-3.5-turbo"):
    """Estima o n√∫mero de tokens em um texto"""
    try:
        encoding = tiktoken.encoding_for_model(model)
        return len(encoding.encode(text))
    except:
        # Estimativa simples se tiktoken falhar
        return len(text) // 4

def split_text_into_chunks(text, max_chunk_tokens=6000, overlap=500):
    """Divide o texto em chunks menores com sobreposi√ß√£o"""
    # Dividir por p√°ginas primeiro (respeitando os marcadores "--- P√°gina X ---")
    pages = []
    current_page = ""
    lines = text.split("\n")
    
    for line in lines:
        if line.startswith("--- P√°gina "):
            if current_page:
                pages.append(current_page)
            current_page = line + "\n"
        else:
            current_page += line + "\n"
    
    if current_page:
        pages.append(current_page)
    
    # Agora dividir as p√°ginas em chunks se necess√°rio
    chunks = []
    current_chunk = ""
    current_chunk_tokens = 0
    
    for page in pages:
        page_tokens = estimate_tokens(page)
        
        # Se a p√°gina inteira couber no chunk atual
        if current_chunk_tokens + page_tokens <= max_chunk_tokens:
            current_chunk += page
            current_chunk_tokens += page_tokens
        # Se a p√°gina for maior que o limite por si s√≥
        elif page_tokens > max_chunk_tokens:
            # Dividir a p√°gina em par√°grafos
            paragraphs = page.split("\n\n")
            for para in paragraphs:
                para_tokens = estimate_tokens(para)
                
                # Se o par√°grafo couber no chunk atual
                if current_chunk_tokens + para_tokens <= max_chunk_tokens:
                    current_chunk += para + "\n\n"
                    current_chunk_tokens += para_tokens
                # Se o par√°grafo for muito grande
                else:
                    # Se tivermos conte√∫do no chunk atual, salve-o
                    if current_chunk:
                        chunks.append(current_chunk)
                    
                    # Se o par√°grafo for maior que o limite, divida-o em senten√ßas
                    if para_tokens > max_chunk_tokens:
                        sentences = para.replace(". ", ".\n").split("\n")
                        current_chunk = ""
                        current_chunk_tokens = 0
                        
                        for sentence in sentences:
                            sentence_tokens = estimate_tokens(sentence)
                            if current_chunk_tokens + sentence_tokens <= max_chunk_tokens:
                                current_chunk += sentence + " "
                                current_chunk_tokens += sentence_tokens
                            else:
                                chunks.append(current_chunk)
                                current_chunk = sentence + " "
                                current_chunk_tokens = sentence_tokens
                    else:
                        current_chunk = para + "\n\n"
                        current_chunk_tokens = para_tokens
        else:
            # Salvar o chunk atual e come√ßar um novo com esta p√°gina
            chunks.append(current_chunk)
            current_chunk = page
            current_chunk_tokens = page_tokens
    
    # Adicionar o √∫ltimo chunk se tiver conte√∫do
    if current_chunk:
        chunks.append(current_chunk)
    
    # Adicionar sobreposi√ß√£o para garantir continuidade
    if overlap > 0 and len(chunks) > 1:
        chunks_with_overlap = [chunks[0]]
        for i in range(1, len(chunks)):
            prev_chunk = chunks[i-1]
            this_chunk = chunks[i]
            
            # Adicionar as √∫ltimas linhas do chunk anterior
            prev_lines = prev_chunk.split("\n")
            overlap_text = "\n".join(prev_lines[-min(len(prev_lines), overlap//10):]) + "\n"
            
            chunks_with_overlap.append(overlap_text + this_chunk)
        
        chunks = chunks_with_overlap
    
    return chunks

def query_ai(query):
    """
    Processa uma consulta usando a API da OpenAI com suporte a documentos grandes
    atrav√©s de chunking (divis√£o em partes menores).
    """
    try:
        # Importar OpenAI dentro da fun√ß√£o para evitar erros de escopo
        import openai
        
        # Cliente OpenAI para v1.0+
        client = openai.OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
        
        # Obter o texto completo do PDF
        full_text = st.session_state.pdf_text
        
        # Instru√ß√£o do sistema refinada para melhor processamento de termos espec√≠ficos
        system_prompt = """
        Voc√™ √© um assistente de IA especializado em analisar o conte√∫do de um documento PDF fornecido e responder perguntas exclusivamente com base nesse conte√∫do.
        
        Instru√ß√µes Importantes:
        1. Sua resposta DEVE ser estritamente baseada nas informa√ß√µes contidas no texto do documento fornecido.
        2. N√ÉO utilize conhecimento externo ou informa√ß√µes que n√£o estejam presentes no documento.
        3. Se a pergunta se referir a um t√≥pico ou termo espec√≠fico (ex: 'undercar', 'telefone', '0800'), procure cuidadosamente por todas as men√ß√µes desse t√≥pico no documento completo antes de responder.
        4. Se a informa√ß√£o solicitada estiver presente, forne√ßa-a de forma clara e cite a parte relevante do texto, se poss√≠vel.
        5. Se a informa√ß√£o solicitada N√ÉO estiver presente no documento, declare explicitamente que a informa√ß√£o n√£o foi encontrada no documento fornecido.
        6. Se partes do documento parecerem incompletas ou amb√≠guas em rela√ß√£o √† pergunta, mencione isso.
        7. Voc√™ receber√° o documento em partes. Considere todas as partes ao formular sua resposta final.
        """
        
        # Calcula o limite de tokens com base no modelo
        model_max_tokens = {
            "gpt-3.5-turbo": 4000,
            "gpt-3.5-turbo-16k": 16000,
            "gpt-4": 8000,
            "gpt-4-32k": 32000,
            "gpt-4-turbo": 128000,
            "gpt-4o": 128000
        }
        
        max_context_tokens = model_max_tokens.get(st.session_state.model, 4000)
        max_chunk_tokens = max(max_context_tokens // 2, 2000)  # Use half of the model's capacity for each chunk
        
        # Estimar tokens no texto completo
        total_tokens = estimate_tokens(full_text, st.session_state.model)
        
        # Tentar primeiro com o documento completo se for pequeno o suficiente
        if total_tokens < max_context_tokens * 0.7:  # Deixar margem de 30%
            st.session_state.processing_status = "Processando documento completo..."
            try:
                response = client.chat.completions.create(
                    model=st.session_state.model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": f"Com base EXCLUSIVAMENTE no seguinte documento, responda √† pergunta: '{query}'\n\nConte√∫do do Documento:\n{full_text}"}
                    ],
                    temperature=0.2,
                    max_tokens=1000
                )
                
                return response.choices[0].message.content
            except openai.BadRequestError as single_error:
                if "context_length_exceeded" not in str(single_error):
                    raise single_error
                # Se der erro de contexto, continuar com chunking
                st.info("O documento √© grande demais para processar de uma s√≥ vez. Dividindo em partes menores...")
        else:
            st.info(f"Documento grande detectado ({total_tokens} tokens estimados). Dividindo em partes menores para processamento...")
        
        # Dividir o texto em chunks
        chunks = split_text_into_chunks(full_text, max_chunk_tokens=max_chunk_tokens)
        
        st.session_state.processing_status = f"Documento dividido em {len(chunks)} partes para processamento."
        
        # Lista para armazenar os resultados parciais
        partial_results = []
        
        # Processar cada chunk
        for i, chunk in enumerate(chunks):
            st.session_state.processing_status = f"Processando parte {i+1} de {len(chunks)}..."
            
            try:
                # Chamada da API para cada chunk
                partial_response = client.chat.completions.create(
                    model=st.session_state.model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": f"Com base EXCLUSIVAMENTE na seguinte PARTE {i+1} de {len(chunks)} do documento, analise se h√° informa√ß√µes relevantes para responder √† pergunta: '{query}'\n\nSe encontrar informa√ß√µes relevantes, forne√ßa-as. Se n√£o encontrar, apenas diga 'N√£o encontrei informa√ß√µes relevantes nesta parte'. N√£o fa√ßa suposi√ß√µes ou use conhecimento externo.\n\nConte√∫do da Parte {i+1}:\n{chunk}"}
                    ],
                    temperature=0.2,
                    max_tokens=500
                )
                
                result = partial_response.choices[0].message.content
                
                # Ignorar resultados sem informa√ß√µes relevantes
                if "N√£o encontrei informa√ß√µes relevantes nesta parte" not in result:
                    partial_results.append({
                        "part": i+1,
                        "content": result
                    })
                    
            except Exception as chunk_error:
                st.warning(f"Erro ao processar parte {i+1}. Continuando com as pr√≥ximas partes. Erro: {str(chunk_error)}")
        
        # Se n√£o encontrou informa√ß√µes relevantes em nenhuma parte
        if not partial_results:
            st.session_state.processing_status = "Conclu√≠do. Nenhuma informa√ß√£o relevante encontrada."
            return f"Ap√≥s analisar todas as {len(chunks)} partes do documento, n√£o encontrei informa√ß√µes relevantes para responder √† pergunta: '{query}'. A informa√ß√£o solicitada n√£o parece estar presente no documento fornecido."
        
        # Combinar os resultados parciais para uma resposta final
        st.session_state.processing_status = "Sintetizando resultados das partes analisadas..."
        
        synthesis_prompt = f"""
        Eu analisei um documento em {len(chunks)} partes diferentes procurando informa√ß√µes para responder √† pergunta: '{query}'
        
        Encontrei informa√ß√µes relevantes nas seguintes partes do documento:
        
        {'\n\n'.join([f"PARTE {r['part']}:\n{r['content']}" for r in partial_results])}
        
        Com base APENAS nestas informa√ß√µes encontradas no documento, forne√ßa uma resposta completa, coerente e concisa para a pergunta original. Se houver conflitos ou ambiguidades nas diferentes partes, mencione-os. Se as informa√ß√µes encontradas forem insuficientes, indique isso claramente.
        """
        
        # Chamada final da API para sintetizar os resultados
        try:
            final_response = client.chat.completions.create(
                model=st.session_state.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": synthesis_prompt}
                ],
                temperature=0.2,
                max_tokens=1000
            )
            
            st.session_state.processing_status = "Conclu√≠do com sucesso."
            return final_response.choices[0].message.content
            
        except Exception as synthesis_error:
            # Se a s√≠ntese falhar, retornar os resultados parciais formatados
            st.session_state.processing_status = "Erro na s√≠ntese final. Retornando resultados parciais."
            combined_results = f"Encontrei as seguintes informa√ß√µes relevantes no documento para responder √† pergunta '{query}':\n\n"
            for r in partial_results:
                combined_results += f"--- Da parte {r['part']} do documento ---\n{r['content']}\n\n"
            
            return combined_results

    except openai.BadRequestError as e:
        # Capturar erro espec√≠fico de contexto muito longo
        if "context_length_exceeded" in str(e):
            st.error(f"Erro: O documento ainda √© muito longo mesmo ap√≥s a divis√£o em partes. Tente usar um documento menor ou selecionar um modelo com maior capacidade de contexto.")
            st.session_state.processing_status = "Erro: documento muito grande."
            return "Erro: O documento excede a capacidade de processamento mesmo com a t√©cnica de divis√£o em partes."
        else:
            st.error(f"Erro na API OpenAI: {str(e)}")
            st.session_state.processing_status = f"Erro na API: {str(e)}"
            return f"Ocorreu um erro ao processar sua consulta com a OpenAI: {str(e)}"
            
    except Exception as e:
        st.error(f"Erro inesperado ao processar consulta: {str(e)}")
        st.session_state.processing_status = f"Erro inesperado: {str(e)}"
        return f"Ocorreu um erro inesperado: {str(e)}"

def verificar_termos_no_pdf(termos, texto_pdf=None):
    """
    Verifica se determinados termos est√£o presentes no texto do PDF
    e retorna suas posi√ß√µes no texto.
    
    Args:
        termos (list): Lista de termos a serem verificados
        texto_pdf (str, optional): Texto do PDF. Se None, usa st.session_state.pdf_text
        
    Returns:
        dict: Dicion√°rio com os termos encontrados e suas posi√ß√µes
    """
    if texto_pdf is None:
        if "pdf_text" not in st.session_state:
            return {"erro": "Texto do PDF n√£o dispon√≠vel"}
        texto_pdf = st.session_state.pdf_text
    
    resultados = {}
    
    for termo in termos:
        termo = termo.lower()
        posicoes = []
        texto_lower = texto_pdf.lower()
        
        # Encontrar todas as ocorr√™ncias
        pos = texto_lower.find(termo)
        while pos != -1:
            # Adicionar contexto (50 caracteres antes e depois)
            inicio = max(0, pos - 50)
            fim = min(len(texto_pdf), pos + len(termo) + 50)
            contexto = texto_pdf[inicio:fim]
            
            posicoes.append({
                "posicao": pos,
                "contexto": contexto
            })
            
            # Procurar pr√≥xima ocorr√™ncia
            pos = texto_lower.find(termo, pos + 1)
        
        if posicoes:
            resultados[termo] = posicoes
    
    return resultados

def diagnosticar_reconhecimento(query, texto_pdf=None):
    """
    Fun√ß√£o de diagn√≥stico para ajudar a identificar problemas de reconhecimento
    de termos espec√≠ficos no PDF.
    
    Args:
        query (str): A consulta do usu√°rio
        texto_pdf (str, optional): Texto do PDF. Se None, usa st.session_state.pdf_text
        
    Returns:
        dict: Resultados do diagn√≥stico
    """
    if texto_pdf is None:
        if "pdf_text" not in st.session_state:
            return {"erro": "Texto do PDF n√£o dispon√≠vel"}
        texto_pdf = st.session_state.pdf_text
    
    # Extrair palavras-chave da consulta (palavras com mais de 3 letras)
    palavras = [p for p in query.lower().split() if len(p) > 3]
    
    # Verificar presen√ßa das palavras-chave no texto
    resultados = verificar_termos_no_pdf(palavras, texto_pdf)
    
    # Adicionar estat√≠sticas gerais
    diagnostico = {
        "tamanho_texto": len(texto_pdf),
        "palavras_analisadas": palavras,
        "palavras_encontradas": list(resultados.keys()),
        "detalhes": resultados
    }
    
    return diagnostico

def export_to_csv():
    """Exporta o hist√≥rico para CSV"""
    if not st.session_state.history:
        st.warning("N√£o h√° consultas para exportar.")
        return None, None
    
    # Criar DataFrame com o hist√≥rico
    data = []
    for item in st.session_state.history:
        data.append({
            "Data e Hora": item.get("timestamp", ""),
            "Pergunta": item.get("query", ""),
            "Resposta": item.get("answer", ""),
            "Modelo": item.get("model", "")
        })
    
    df = pd.DataFrame(data)
    csv = df.to_csv(index=False).encode('utf-8')
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"oraculo_historico_{timestamp}.csv"
    
    return csv, filename

def show_extracted_text():
    """Exibe o texto extra√≠do para depura√ß√£o"""
    if st.session_state.pdf_text:
        st.text_area("Texto Completo Extra√≠do do PDF", st.session_state.pdf_text, height=300)
        
        # Adicionar ferramenta de diagn√≥stico para termos espec√≠ficos
        st.subheader("Diagn√≥stico de Termos")
        termo_busca = st.text_input("Digite um termo para verificar no PDF:")
        if termo_busca and st.button("Verificar Termo"):
            resultados = verificar_termos_no_pdf([termo_busca])
            if termo_busca.lower() in resultados:
                st.success(f"Termo '{termo_busca}' encontrado {len(resultados[termo_busca.lower()])} vezes no documento!")
                for i, ocorrencia in enumerate(resultados[termo_busca.lower()]):
                    st.markdown(f"**Ocorr√™ncia {i+1}:** (posi√ß√£o {ocorrencia['posicao']})")
                    st.text(f"...{ocorrencia['contexto']}...")
            else:
                st.error(f"Termo '{termo_busca}' n√£o encontrado no documento.")
        
        # Diagn√≥stico de chunking
        st.subheader("Diagn√≥stico de Chunking")
        if st.button("Testar Divis√£o em Chunks"):
            total_tokens = estimate_tokens(st.session_state.pdf_text)
            st.write(f"Tamanho estimado do documento: {total_tokens} tokens")
            
            # Testar divis√£o com diferentes limites
            for max_tokens in [2000, 4000, 6000]:
                chunks = split_text_into_chunks(st.session_state.pdf_text, max_chunk_tokens=max_tokens)
                st.write(f"Com limite de {max_tokens} tokens por chunk: {len(chunks)} chunks gerados")
                
                # Mostrar amostra do primeiro chunk
                if chunks:
                    with st.expander(f"Amostra do primeiro chunk ({estimate_tokens(chunks[0])} tokens)"):
                        st.text(chunks[0][:500] + "...")
    else:
        st.warning("Nenhum texto extra√≠do ainda.")

# Validar ambiente na inicializa√ß√£o
validate_environment()

# Interface principal
st.title("üöó Or√°culo - Sistema de Consulta Inteligente")

# Barra lateral com valida√ß√µes e configura√ß√µes
with st.sidebar:
    st.header("‚öôÔ∏è Valida√ß√£o do Sistema")
    
    # Status de valida√ß√£o da chave OpenAI
    openai_status = "‚úÖ" if VALIDATION_MESSAGES.get("openai_key") == "OK" else "‚ùå"
    st.write(f"{openai_status} **Chave OpenAI**: {VALIDATION_MESSAGES.get('openai_key')}")
    
    # Status de valida√ß√£o do PDF
    pdf_status = "‚úÖ" if VALIDATION_MESSAGES.get("pdf_exists") == "OK" else "‚ùå"
    st.write(f"{pdf_status} **PDF Carregado**: {VALIDATION_MESSAGES.get('pdf_exists')}")
    
    # Status de extra√ß√£o de texto
    text_status = "‚úÖ" if VALIDATION_MESSAGES.get("text_extraction") == "OK" else "‚ùå"
    st.write(f"{text_status} **Texto Extra√≠do**: {VALIDATION_MESSAGES.get('text_extraction')}")
    
    # Status de depend√™ncias
    dep_status = "‚úÖ" if VALIDATION_MESSAGES.get("dependencies") == "OK" else "‚ùå"
    st.write(f"{dep_status} **Depend√™ncias**: {VALIDATION_MESSAGES.get('dependencies')}")
    
    # Informa√ß√µes do PDF
    if VALIDATION_OK:
        st.divider()
        st.subheader("üìÑ Informa√ß√µes do PDF")
        st.write(f"Arquivo: {os.path.basename(PDF_PATH)}")
        st.write(f"Tamanho do texto: {len(st.session_state.pdf_text)} caracteres")
        st.write(f"Tokens estimados: {estimate_tokens(st.session_state.pdf_text)}")
        
        # Configura√ß√µes de modelo - ATUALIZADO COM MAIS OP√á√ïES
        st.divider()
        st.subheader("‚öôÔ∏è Configura√ß√µes")
        
        model = st.selectbox(
            "Modelo OpenAI:",
            options=[
                "gpt-3.5-turbo",      # Modelo b√°sico, bom custo-benef√≠cio
                "gpt-4",              # Melhor qualidade, mais caro
                "gpt-4-turbo",        # Melhor performance que GPT-4 com custo menor
                "gpt-4o",             # GPT-4 Omni - modelo mais recente
                "gpt-3.5-turbo-16k",  # Vers√£o com contexto maior
                "gpt-4-32k",          # GPT-4 com contexto de 32k tokens
            ],
            index=0,  # Default para o modelo mais econ√¥mico
            help="Selecione o modelo da OpenAI. GPT-3.5 √© mais r√°pido e econ√¥mico, GPT-4 √© mais preciso, modelos com n√∫mero maior suportam mais contexto."
        )
        
        # Atualizar o modelo selecionado
        if model != st.session_state.model:
            st.session_state.model = model
            st.info(f"Modelo alterado para {model}")
        
        # Bot√£o para exportar hist√≥rico
        st.divider()
        st.subheader("üìä Exportar Hist√≥rico")
        
        export_button = st.button("üì• Exportar Consultas para CSV")
        if export_button:
            csv_data, filename = export_to_csv()
            if csv_data is not None and filename is not None:
                st.download_button(
                    label="‚¨áÔ∏è Baixar CSV",
                    data=csv_data,
                    file_name=filename,
                    mime="text/csv",
                )
        
        # Modo de depura√ß√£o para visualizar o texto extra√≠do
        st.divider()
        st.subheader("üîç Depura√ß√£o")
        if st.button("Visualizar Texto Extra√≠do"):
            show_extracted_text()

# Conte√∫do principal - Sempre exibir, independente da valida√ß√£o
st.write("Digite sua pergunta sobre ve√≠culos e clique em 'Consultar'.")

# Campo de consulta
query = st.text_input("‚ùì Sua pergunta:", key="query_input")

# Status do processamento
if st.session_state.processing_status:
    st.info(st.session_state.processing_status)

# Bot√£o de consulta
consult_button = st.button("üîç Consultar", key="query_button", disabled=not VALIDATION_OK)
if consult_button and query:
    st.session_state.processing_status = "Iniciando processamento..."
    with st.spinner("Processando consulta..."):
        answer = query_ai(query)
        
        if answer:
            st.divider()
            st.subheader("üìù Resposta:")
            st.markdown(answer)
            
            # Adicionar ao hist√≥rico com timestamp
            st.session_state.history.append({
                "query": query,
                "answer": answer,
                "model": st.session_state.model,
                "timestamp": datetime.now().strftime("%d/%m/%Y %H:%M:%S")
            })
            
            # Resetar status ap√≥s conclus√£o
            st.session_state.processing_status = None

# Hist√≥rico de consultas - sempre mostrar se houver itens
if st.session_state.history:
    st.divider()
    st.subheader("üìã Hist√≥rico de Consultas")
    
    for i, item in enumerate(reversed(st.session_state.history)):
        question = item.get("query", "")
        with st.expander(f"Pergunta ({i+1}): {question}"):
            st.markdown(f"**Data e Hora:** {item.get('timestamp', '')}")
            st.markdown(f"**Modelo:** {item.get('model', '')}")
            st.markdown("**Resposta:**")
            st.markdown(item.get('answer', ''))
